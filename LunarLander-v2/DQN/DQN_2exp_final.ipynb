{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188dc4ff",
   "metadata": {},
   "source": [
    "DQN ROBUSTNESS UNDER STOCHASTICITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23cbb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "import imageio.v2 as imageio\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4bf359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "SEEDS = [0, 42, 1729, 6174, 196]\n",
    "FAILURE_PROBS = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "TRAINING_TIMESTEPS = 700_000\n",
    "N_EVAL_EPISODES = 100\n",
    "\n",
    "# FIX #7: Target update interval set to 10,000\n",
    "# FIX #8: Exploration fraction = 0.2, final eps = 0.05\n",
    "DQN_CONFIG = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'buffer_size': 100_000,\n",
    "    'learning_starts': 10_000,\n",
    "    'batch_size': 64,\n",
    "    'gamma': 0.99,\n",
    "    'train_freq': 4,\n",
    "    'gradient_steps': 1,\n",
    "    'target_update_interval': 10_000,  # FIX #7\n",
    "    'exploration_fraction': 0.2,        # FIX #8\n",
    "    'exploration_initial_eps': 1.0,\n",
    "    'exploration_final_eps': 0.05,      # FIX #8\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'figure.dpi': 150, 'font.size': 11})\n",
    "\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a53ec",
   "metadata": {},
   "source": [
    "### ENVIRONMENT WRAPPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ace237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment wrappers loaded\n"
     ]
    }
   ],
   "source": [
    "class StochasticLunarLanderWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    FIX #2: Single wrapper that implements ONE stochastic family per episode.\n",
    "    At reset, samples Bernoulli(p):\n",
    "    - If false: no failures\n",
    "    - If true: exactly ONE family is activated\n",
    "    \n",
    "    FIX #4: Tracks failure_occurred and failure_step globally.\n",
    "    \"\"\"\n",
    "    \n",
    "    FAMILIES = ['sensor', 'actuator', 'dynamics', 'catastrophic']\n",
    "    \n",
    "    def __init__(self, env, failure_prob=0.0, seed=None):\n",
    "        super().__init__(env)\n",
    "        self.failure_prob = failure_prob\n",
    "        \n",
    "        # FIX #3: Single RNG initialization (no env.env.env crawling)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        \n",
    "        # FIX #2: Active family selection\n",
    "        self.active_family = None\n",
    "        \n",
    "        # FIX #4: Shared failure tracking\n",
    "        self.failure_occurred = False\n",
    "        self.failure_step = None\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Family-specific state\n",
    "        self._reset_family_states()\n",
    "        \n",
    "        # Last observation for sensor dropout\n",
    "        self.last_obs = None\n",
    "    \n",
    "    def _reset_family_states(self):\n",
    "        \"\"\"Reset all family-specific failure states\"\"\"\n",
    "        # Sensor failures\n",
    "        self.altimeter_drift_enabled = False\n",
    "        self.altimeter_bias = 0.0\n",
    "        \n",
    "        # Actuator failures\n",
    "        self.engine_degradation_active = False\n",
    "        self.thrust_multiplier = 1.0\n",
    "        self.thruster_asymmetry_active = False\n",
    "        self.left_mult = 1.0\n",
    "        self.right_mult = 1.0\n",
    "        self.control_lag_active = False\n",
    "        self.action_buffer = []\n",
    "        \n",
    "        # Dynamics failures\n",
    "        self.fuel_leak_active = False\n",
    "        self.leak_rate = 0.0\n",
    "        self.gravity_variation_active = False\n",
    "        self.gravity_mult = 1.0\n",
    "        self.current_mass = 1.0\n",
    "        \n",
    "        # Catastrophic failures\n",
    "        self.engine_cutoff_active = False\n",
    "        self.engine_cutoff_step = -1\n",
    "        self.landing_gear_failure = False\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        # FIX #3: Use seed parameter properly\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        \n",
    "        # FIX #2: Single Bernoulli sampling\n",
    "        self._reset_family_states()\n",
    "        self.current_step = 0\n",
    "        self.failure_occurred = False\n",
    "        self.failure_step = None\n",
    "        self.last_obs = obs.copy()\n",
    "        \n",
    "        if self.rng.random() < self.failure_prob:\n",
    "            # Failure WILL occur - select ONE family\n",
    "            self.active_family = self.rng.choice(self.FAMILIES)\n",
    "            self._initialize_active_family()\n",
    "        else:\n",
    "            # NO failures this episode\n",
    "            self.active_family = None\n",
    "        \n",
    "        info['active_family'] = self.active_family\n",
    "        return obs, info\n",
    "    \n",
    "    def _initialize_active_family(self):\n",
    "        \"\"\"Initialize the selected stochastic family\"\"\"\n",
    "        if self.active_family == 'sensor':\n",
    "            self.altimeter_drift_enabled = True\n",
    "            \n",
    "        elif self.active_family == 'actuator':\n",
    "            # Randomly select one actuator failure mode\n",
    "            mode = self.rng.choice(['degradation', 'asymmetry'])\n",
    "            if mode == 'degradation':\n",
    "                self.engine_degradation_active = True\n",
    "                self.thrust_multiplier = self.rng.uniform(0.3, 0.6)\n",
    "            else:\n",
    "                self.thruster_asymmetry_active = True\n",
    "                weakness = self.rng.uniform(0.4, 0.8)\n",
    "                if self.rng.random() < 0.5:\n",
    "                    self.left_mult, self.right_mult = weakness, 1.0\n",
    "                else:\n",
    "                    self.left_mult, self.right_mult = 1.0, weakness\n",
    "                    \n",
    "        elif self.active_family == 'dynamics':\n",
    "            # Randomly select one dynamics failure mode\n",
    "            mode = self.rng.choice(['fuel_leak', 'gravity'])\n",
    "            if mode == 'fuel_leak':\n",
    "                self.fuel_leak_active = True\n",
    "                self.leak_rate = self.rng.uniform(0.001, 0.003)\n",
    "            else:\n",
    "                self.gravity_variation_active = True\n",
    "                self.gravity_mult = self.rng.uniform(0.9, 1.1)\n",
    "                \n",
    "        elif self.active_family == 'catastrophic':\n",
    "            # Randomly select one catastrophic failure mode\n",
    "            mode = self.rng.choice(['cutoff', 'landing_gear'])\n",
    "            if mode == 'cutoff':\n",
    "                self.engine_cutoff_active = True\n",
    "                self.engine_cutoff_step = self.rng.randint(100, 600)\n",
    "            else:\n",
    "                self.landing_gear_failure = True\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        \"\"\"Apply sensor failures if active\"\"\"\n",
    "        if self.active_family != 'sensor':\n",
    "            return obs\n",
    "        \n",
    "        obs = obs.copy()\n",
    "        \n",
    "        # FIX #4: Mark first failure\n",
    "        if not self.failure_occurred:\n",
    "            self.failure_occurred = True\n",
    "            self.failure_step = self.current_step\n",
    "        \n",
    "        # Altimeter drift (episodic + stepwise)\n",
    "        if self.altimeter_drift_enabled:\n",
    "            self.altimeter_bias += self.rng.uniform(-0.02, 0.02)\n",
    "            obs[1] += self.altimeter_bias\n",
    "        \n",
    "        # Gyroscope malfunction (stepwise)\n",
    "        if self.rng.random() < 0.3:\n",
    "            obs[4] *= self.rng.choice([0.5, 1.5, -1.0])\n",
    "        \n",
    "        # Velocity sensor error (stepwise)\n",
    "        if self.rng.random() < 0.3:\n",
    "            obs[2] += self.rng.uniform(-1.0, 1.0)\n",
    "        \n",
    "        # Sensor dropout (stepwise)\n",
    "        if self.rng.random() < 0.3:\n",
    "            dropout_idx = self.rng.choice([0, 1, 4])\n",
    "            if self.rng.random() < 0.5:\n",
    "                obs[dropout_idx] = 0.0\n",
    "            elif self.last_obs is not None:\n",
    "                obs[dropout_idx] = self.last_obs[dropout_idx]\n",
    "        \n",
    "        self.last_obs = obs.copy()\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Apply actuator failures\n",
    "        if self.active_family == 'actuator':\n",
    "            action = self._apply_actuator_failures(action)\n",
    "        \n",
    "        # Apply catastrophic failures\n",
    "        if self.active_family == 'catastrophic':\n",
    "            action = self._apply_catastrophic_failures(action)\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Apply observation failures\n",
    "        obs = self.observation(obs)\n",
    "        \n",
    "        # Apply dynamics failures\n",
    "        if self.active_family == 'dynamics':\n",
    "            self._apply_dynamics_failures(action)\n",
    "        \n",
    "        # Apply physics modifications\n",
    "        if self.active_family == 'actuator':\n",
    "            self._apply_actuator_physics()\n",
    "        \n",
    "        # Handle catastrophic landing gear failure\n",
    "        if self.active_family == 'catastrophic' and self.landing_gear_failure:\n",
    "            if terminated and reward > 0:\n",
    "                reward = -100\n",
    "                info['landing_gear_crash'] = True\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def _apply_actuator_failures(self, action):\n",
    "        \"\"\"Apply actuator failures to action\"\"\"\n",
    "        # FIX #4: Mark first failure\n",
    "        if not self.failure_occurred:\n",
    "            self.failure_occurred = True\n",
    "            self.failure_step = self.current_step\n",
    "        \n",
    "        # Control lag\n",
    "        if not self.control_lag_active and self.rng.random() < 0.1:\n",
    "            self.control_lag_active = True\n",
    "            self.action_buffer = []\n",
    "        \n",
    "        if self.control_lag_active:\n",
    "            self.action_buffer.append(action)\n",
    "            if len(self.action_buffer) > self.rng.randint(1, 4):\n",
    "                action = self.action_buffer.pop(0)\n",
    "            else:\n",
    "                action = 0\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _apply_actuator_physics(self):\n",
    "        \"\"\"Apply actuator physics modifications\"\"\"\n",
    "        try:\n",
    "            lander = self.env.unwrapped.lander\n",
    "            if lander is None:\n",
    "                return\n",
    "            \n",
    "            if self.engine_degradation_active:\n",
    "                vel = lander.linearVelocity\n",
    "                if vel.y > 0:\n",
    "                    lander.linearVelocity = (\n",
    "                        vel.x, \n",
    "                        vel.y * (1.0 - (1.0 - self.thrust_multiplier) * 0.15)\n",
    "                    )\n",
    "            \n",
    "            if self.thruster_asymmetry_active:\n",
    "                ang_vel = lander.angularVelocity\n",
    "                if self.left_mult < 1.0 and ang_vel > 0:\n",
    "                    lander.angularVelocity = ang_vel * self.left_mult\n",
    "                elif self.right_mult < 1.0 and ang_vel < 0:\n",
    "                    lander.angularVelocity = ang_vel * self.right_mult\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def _apply_dynamics_failures(self, action):\n",
    "        \"\"\"Apply dynamics failures\"\"\"\n",
    "        # FIX #4: Mark first failure\n",
    "        if not self.failure_occurred:\n",
    "            self.failure_occurred = True\n",
    "            self.failure_step = self.current_step\n",
    "        \n",
    "        try:\n",
    "            lander = self.env.unwrapped.lander\n",
    "            if lander is None:\n",
    "                return\n",
    "            \n",
    "            # Mass depletion\n",
    "            if action == 2:\n",
    "                self.current_mass -= 0.001\n",
    "                self.current_mass = max(0.5, self.current_mass)\n",
    "            \n",
    "            # Fuel leak\n",
    "            if self.fuel_leak_active:\n",
    "                self.current_mass -= self.leak_rate\n",
    "                self.current_mass = max(0.5, self.current_mass)\n",
    "                if action == 2:\n",
    "                    vel = lander.linearVelocity\n",
    "                    lander.linearVelocity = (vel.x, vel.y * 0.95)\n",
    "            \n",
    "            # Gravity variation\n",
    "            if self.gravity_variation_active:\n",
    "                vel = lander.linearVelocity\n",
    "                correction = (self.gravity_mult - 1.0) * 0.05\n",
    "                lander.linearVelocity = (vel.x, vel.y - correction)\n",
    "            \n",
    "            # Wind turbulence\n",
    "            if self.rng.random() < 0.2:\n",
    "                wind = self.rng.uniform(-2.0, 2.0)\n",
    "                vel = lander.linearVelocity\n",
    "                lander.linearVelocity = (vel.x + wind * 0.1, vel.y)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def _apply_catastrophic_failures(self, action):\n",
    "        \"\"\"Apply catastrophic failures\"\"\"\n",
    "        # FIX #4: Mark first failure at cutoff point\n",
    "        if self.engine_cutoff_active and self.current_step >= self.engine_cutoff_step:\n",
    "            if not self.failure_occurred:\n",
    "                self.failure_occurred = True\n",
    "                self.failure_step = self.current_step\n",
    "            if action == 2:\n",
    "                action = 0\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "def make_stochastic_lunar_lander(failure_prob, seed=None):\n",
    "    \"\"\"\n",
    "    FIX #1: Create single environment instance (NO vectorization)\n",
    "    FIX #3: Proper seed handling\n",
    "    \"\"\"\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env = StochasticLunarLanderWrapper(env, failure_prob, seed)\n",
    "    \n",
    "    # FIX #3: Use reset with seed parameter (no manual crawling)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "    \n",
    "    return env\n",
    "\n",
    "print(\"✓ Environment wrappers loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b254373",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d52ba85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=10000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "    \n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "\n",
    "def train_dqn_agent(failure_prob, seed, save_dir, verbose=1):\n",
    "    \"\"\"\n",
    "    FIX #1: Train on SINGLE environment (no vectorization)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining DQN | p={failure_prob:.2f} | seed={seed}\")\n",
    "    \n",
    "    # FIX #1: Single environment wrapped in Monitor\n",
    "    env = Monitor(make_stochastic_lunar_lander(failure_prob, seed))\n",
    "    model = DQN('MlpPolicy', env, seed=seed, **DQN_CONFIG)\n",
    "    \n",
    "    callback = TrainingProgressCallback(10000, verbose)\n",
    "    model.learn(TRAINING_TIMESTEPS, callback=callback, progress_bar=(verbose > 0))\n",
    "    \n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = save_dir / f\"dqn_p{int(failure_prob*100):02d}_seed{seed}.zip\"\n",
    "    model.save(str(model_path))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✓ Saved: {model_path}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_experiment1(output_dir=Path(\"models/exp1\"), verbose=1):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT 1: Training 30 agents (6 levels × 5 seeds)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    models = {}\n",
    "    for p in FAILURE_PROBS:\n",
    "        for seed in SEEDS:\n",
    "            models[(p, seed)] = train_dqn_agent(p, seed, output_dir, verbose)\n",
    "    \n",
    "    print(\"\\n✓ Experiment 1 training complete\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_experiment2(output_dir=Path(\"models/exp2\"), verbose=1):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT 2: Training 5 agents (p=0.0 only)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    models = {}\n",
    "    for seed in SEEDS:\n",
    "        models[(0.0, seed)] = train_dqn_agent(0.0, seed, output_dir, verbose)\n",
    "    \n",
    "    print(\"\\n✓ Experiment 2 training complete\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "864cb543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training module loaded\n"
     ]
    }
   ],
   "source": [
    "def load_all_models_exp1(model_dir=Path(\"models/exp1\")):\n",
    "    models = {}\n",
    "    for p in FAILURE_PROBS:\n",
    "        for seed in SEEDS:\n",
    "            path = model_dir / f\"dqn_p{int(p*100):02d}_seed{seed}.zip\"\n",
    "            if path.exists():\n",
    "                models[(p, seed)] = DQN.load(str(path))\n",
    "    print(f\"✓ Loaded {len(models)}/30 models\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def load_all_models_exp2(model_dir=Path(\"models/exp2\")):\n",
    "    models = {}\n",
    "    for seed in SEEDS:\n",
    "        path = model_dir / f\"dqn_p00_seed{seed}.zip\"\n",
    "        if path.exists():\n",
    "            models[(0.0, seed)] = DQN.load(str(path))\n",
    "    print(f\"✓ Loaded {len(models)}/5 models\")\n",
    "    return models\n",
    "\n",
    "print(\"✓ Training module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b51d61",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db65ed7",
   "metadata": {},
   "source": [
    "Separate metrics for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce337b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation module loaded\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Exp1EpisodeMetrics:\n",
    "    \"\"\"FIX #6: Experiment 1 metrics only\"\"\"\n",
    "    episode_id: int\n",
    "    seed: int\n",
    "    failure_prob: float\n",
    "    total_return: float\n",
    "    episode_length: int\n",
    "    success: bool\n",
    "    crash: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Exp2EpisodeMetrics:\n",
    "    \"\"\"FIX #6: Experiment 2 metrics (includes failure tracking)\"\"\"\n",
    "    episode_id: int\n",
    "    seed: int\n",
    "    failure_prob: float\n",
    "    total_return: float\n",
    "    episode_length: int\n",
    "    success: bool\n",
    "    crash: bool\n",
    "    failure_occurred: bool\n",
    "    time_to_failure: int  # FIX #5: Steps until first failure\n",
    "    pre_fault_return: float\n",
    "    post_fault_return: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Exp1AggregatedMetrics:\n",
    "    \"\"\"FIX #6: Experiment 1 aggregated metrics\"\"\"\n",
    "    failure_prob: float\n",
    "    seed: int\n",
    "    n_episodes: int\n",
    "    mean_return: float\n",
    "    std_return: float\n",
    "    success_rate: float\n",
    "    crash_rate: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Exp2AggregatedMetrics:\n",
    "    \"\"\"FIX #6: Experiment 2 aggregated metrics\"\"\"\n",
    "    failure_prob: float\n",
    "    seed: int\n",
    "    n_episodes: int\n",
    "    mean_return: float\n",
    "    std_return: float\n",
    "    success_rate: float\n",
    "    crash_rate: float\n",
    "    mean_time_to_failure: float  # FIX #5\n",
    "    mean_pre_fault_return: float\n",
    "    mean_post_fault_return: float\n",
    "\n",
    "\n",
    "def evaluate_single_episode_exp1(model, env, ep_id, seed, failure_prob):\n",
    "    \"\"\"\n",
    "    FIX #6: Experiment 1 evaluation (no failure tracking)\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    ret, length = 0.0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        ret += reward\n",
    "        length += 1\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    metrics = Exp1EpisodeMetrics(\n",
    "        episode_id=ep_id,\n",
    "        seed=seed,\n",
    "        failure_prob=failure_prob,\n",
    "        total_return=ret,\n",
    "        episode_length=length,\n",
    "        success=(ret >= 200),\n",
    "        crash=(ret < -100)\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_single_episode_exp2(model, env, ep_id, seed, failure_prob, record=False):\n",
    "    \"\"\"\n",
    "    FIX #4: Track failure_occurred and failure_step from environment\n",
    "    FIX #5: Compute time_to_failure correctly\n",
    "    FIX #6: Experiment 2 evaluation (full metrics)\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    ret, length = 0.0, 0\n",
    "    pre_ret, post_ret = 0.0, 0.0\n",
    "    frames = [] if record else None\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if record:\n",
    "            frame = env.render()\n",
    "            if frame is not None:\n",
    "                frames.append(frame)\n",
    "        \n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        ret += reward\n",
    "        length += 1\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # FIX #4: Use environment's failure tracking\n",
    "        if env.failure_occurred and env.failure_step is not None:\n",
    "            if length <= env.failure_step:\n",
    "                pre_ret += reward\n",
    "            else:\n",
    "                post_ret += reward\n",
    "        else:\n",
    "            pre_ret += reward\n",
    "    \n",
    "    # FIX #5: Time-to-failure = steps until first failure (or episode length if none)\n",
    "    if env.failure_occurred and env.failure_step is not None:\n",
    "        time_to_failure = env.failure_step\n",
    "    else:\n",
    "        time_to_failure = length\n",
    "    \n",
    "    metrics = Exp2EpisodeMetrics(\n",
    "        episode_id=ep_id,\n",
    "        seed=seed,\n",
    "        failure_prob=failure_prob,\n",
    "        total_return=ret,\n",
    "        episode_length=length,\n",
    "        success=(ret >= 200),\n",
    "        crash=(ret < -100),\n",
    "        failure_occurred=env.failure_occurred,\n",
    "        time_to_failure=time_to_failure,\n",
    "        pre_fault_return=pre_ret,\n",
    "        post_fault_return=post_ret\n",
    "    )\n",
    "    \n",
    "    return metrics, frames\n",
    "\n",
    "\n",
    "def evaluate_agent_exp1(model, failure_prob, seed, n_episodes=100):\n",
    "    \"\"\"FIX #6: Experiment 1 evaluation\"\"\"\n",
    "    env = make_stochastic_lunar_lander(failure_prob, seed)\n",
    "    \n",
    "    metrics_list = []\n",
    "    for ep in range(n_episodes):\n",
    "        metrics = evaluate_single_episode_exp1(model, env, ep, seed, failure_prob)\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    env.close()\n",
    "    return metrics_list\n",
    "\n",
    "\n",
    "def evaluate_agent_exp2(model, failure_prob, seed, n_episodes=100, record_last=0):\n",
    "    \"\"\"FIX #6: Experiment 2 evaluation (with failure tracking)\"\"\"\n",
    "    env = make_stochastic_lunar_lander(failure_prob, seed)\n",
    "    \n",
    "    if record_last > 0:\n",
    "        env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "        env = StochasticLunarLanderWrapper(env, failure_prob, seed)\n",
    "        env.reset(seed=seed)\n",
    "    \n",
    "    metrics_list = []\n",
    "    all_frames = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        should_record = (ep >= n_episodes - record_last) and record_last > 0\n",
    "        metrics, frames = evaluate_single_episode_exp2(\n",
    "            model, env, ep, seed, failure_prob, should_record\n",
    "        )\n",
    "        metrics_list.append(metrics)\n",
    "        if frames:\n",
    "            all_frames.extend(frames)\n",
    "    \n",
    "    env.close()\n",
    "    return metrics_list, (all_frames if all_frames else None)\n",
    "\n",
    "\n",
    "def aggregate_metrics_exp1(metrics_list):\n",
    "    \"\"\"FIX #6: Experiment 1 aggregation\"\"\"\n",
    "    returns = [m.total_return for m in metrics_list]\n",
    "    successes = [m.success for m in metrics_list]\n",
    "    crashes = [m.crash for m in metrics_list]\n",
    "    \n",
    "    return Exp1AggregatedMetrics(\n",
    "        failure_prob=metrics_list[0].failure_prob,\n",
    "        seed=metrics_list[0].seed,\n",
    "        n_episodes=len(metrics_list),\n",
    "        mean_return=np.mean(returns),\n",
    "        std_return=np.std(returns),\n",
    "        success_rate=np.mean(successes),\n",
    "        crash_rate=np.mean(crashes)\n",
    "    )\n",
    "\n",
    "\n",
    "def aggregate_metrics_exp2(metrics_list):\n",
    "    \"\"\"FIX #6: Experiment 2 aggregation (includes failure metrics)\"\"\"\n",
    "    returns = [m.total_return for m in metrics_list]\n",
    "    successes = [m.success for m in metrics_list]\n",
    "    crashes = [m.crash for m in metrics_list]\n",
    "    ttf = [m.time_to_failure for m in metrics_list]\n",
    "    pre_rets = [m.pre_fault_return for m in metrics_list]\n",
    "    post_rets = [m.post_fault_return for m in metrics_list]\n",
    "    \n",
    "    return Exp2AggregatedMetrics(\n",
    "        failure_prob=metrics_list[0].failure_prob,\n",
    "        seed=metrics_list[0].seed,\n",
    "        n_episodes=len(metrics_list),\n",
    "        mean_return=np.mean(returns),\n",
    "        std_return=np.std(returns),\n",
    "        success_rate=np.mean(successes),\n",
    "        crash_rate=np.mean(crashes),\n",
    "        mean_time_to_failure=np.mean(ttf),\n",
    "        mean_pre_fault_return=np.mean(pre_rets),\n",
    "        mean_post_fault_return=np.mean(post_rets)\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_experiment1(models, n_episodes=100, output_dir=Path(\"results/exp1\")):\n",
    "    \"\"\"FIX #6: Experiment 1 evaluation pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT 1: Cross-evaluation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for (train_p, train_seed), model in models.items():\n",
    "        print(f\"\\nModel trained at p={train_p:.2f}, seed={train_seed}\")\n",
    "        for test_p in FAILURE_PROBS:\n",
    "            print(f\"  Testing at p={test_p:.2f}...\", end=\" \")\n",
    "            metrics_list = evaluate_agent_exp1(model, test_p, train_seed, n_episodes)\n",
    "            agg = aggregate_metrics_exp1(metrics_list)\n",
    "            \n",
    "            result = {\n",
    "                'train_p': train_p,\n",
    "                'test_p': test_p,\n",
    "                'seed': train_seed,\n",
    "                **asdict(agg)\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"✓ (return={agg.mean_return:.1f})\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_path = output_dir / \"exp1_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✓ Saved: {csv_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_experiment2(models, n_episodes=100, record_gifs=True, output_dir=Path(\"results/exp2\")):\n",
    "    \"\"\"\n",
    "    FIX #6: Experiment 2 evaluation pipeline (with failure tracking)\n",
    "    FIX #9: GIF speed = 2× real-time (fps=100)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT 2: Zero-shot evaluation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for (_, seed), model in models.items():\n",
    "        print(f\"\\nModel seed={seed}\")\n",
    "        for test_p in FAILURE_PROBS:\n",
    "            print(f\"  Testing at p={test_p:.2f}...\", end=\" \")\n",
    "            record_n = 10 if record_gifs else 0\n",
    "            metrics_list, frames = evaluate_agent_exp2(\n",
    "                model, test_p, seed, n_episodes, record_n\n",
    "            )\n",
    "            agg = aggregate_metrics_exp2(metrics_list)\n",
    "            \n",
    "            result = {'train_p': 0.0, 'test_p': test_p, 'seed': seed, **asdict(agg)}\n",
    "            results.append(result)\n",
    "            \n",
    "            if frames:\n",
    "                gif_dir = output_dir / \"gifs\"\n",
    "                gif_dir.mkdir(parents=True, exist_ok=True)\n",
    "                gif_path = gif_dir / f\"exp2_p{int(test_p*100):02d}_seed{seed}.gif\"\n",
    "                # FIX #9: 2× real-time playback via fps=100\n",
    "                imageio.mimsave(str(gif_path), frames, fps=100, loop=0)\n",
    "                print(f\"✓ GIF saved (2× speed)\", end=\" \")\n",
    "            \n",
    "            print(f\"(return={agg.mean_return:.1f})\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_path = output_dir / \"exp2_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✓ Saved: {csv_path}\")\n",
    "    return df\n",
    "\n",
    "print(\"✓ Evaluation module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ae745",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6563e8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization module loaded\n"
     ]
    }
   ],
   "source": [
    "def plot_exp1_heatmap(df, metric, output_dir):\n",
    "    pivot = df.groupby(['train_p', 'test_p'])[metric].mean().unstack()\n",
    "    if 'rate' in metric:\n",
    "        pivot *= 100\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='RdYlGn', center=50 if 'rate' in metric else 150,\n",
    "                ax=ax, linewidths=1, cbar_kws={'label': metric.replace('_', ' ').title()})\n",
    "    \n",
    "    ax.set_xlabel('Test Failure Probability')\n",
    "    ax.set_ylabel('Training Failure Probability')\n",
    "    ax.set_title(f'Experiment 1: {metric.replace(\"_\", \" \").title()}')\n",
    "    ax.set_xticklabels([f'{int(p*100)}%' for p in pivot.columns])\n",
    "    ax.set_yticklabels([f'{int(p*100)}%' for p in pivot.index], rotation=0)\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f'exp1_{metric}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_exp1_robustness_curves(df, output_dir):\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    for train_p in sorted(df['train_p'].unique()):\n",
    "        subset = df[df['train_p'] == train_p]\n",
    "        grouped = subset.groupby('test_p')['success_rate'].agg(['mean', 'sem'])\n",
    "        \n",
    "        test_ps = [p * 100 for p in grouped.index]\n",
    "        means = grouped['mean'] * 100\n",
    "        sems = grouped['sem'] * 100\n",
    "        \n",
    "        ax.plot(test_ps, means, marker='o', linewidth=2, markersize=8, \n",
    "                label=f'Train p={int(train_p*100)}%')\n",
    "        ax.fill_between(test_ps, means - 1.96*sems, means + 1.96*sems, alpha=0.2)\n",
    "    \n",
    "    ax.axhline(50, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='50% Threshold')\n",
    "    ax.set_xlabel('Test Failure Probability (%)')\n",
    "    ax.set_ylabel('Success Rate (%)')\n",
    "    ax.set_title('Experiment 1: Robustness Curves')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'exp1_robustness_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_exp2_performance(df, metric, output_dir):\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    grouped = df.groupby('test_p')[metric].agg(['mean', 'sem'])\n",
    "    test_ps = [p * 100 for p in grouped.index]\n",
    "    means = grouped['mean']\n",
    "    sems = grouped['sem']\n",
    "    \n",
    "    if 'rate' in metric:\n",
    "        means *= 100\n",
    "        sems *= 100\n",
    "    \n",
    "    ax.plot(test_ps, means, marker='o', linewidth=3, markersize=10, color='steelblue')\n",
    "    ax.fill_between(test_ps, means - 1.96*sems, means + 1.96*sems, alpha=0.3)\n",
    "    \n",
    "    if 'success_rate' in metric:\n",
    "        ax.axhline(50, color='red', linestyle='--', linewidth=2, label='50% Threshold')\n",
    "    \n",
    "    ax.set_xlabel('Test Failure Probability (%)')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(f'Experiment 2: {metric.replace(\"_\", \" \").title()}')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f'exp2_{metric}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_all_exp1_plots(df, output_dir=Path(\"results/exp1/plots\")):\n",
    "    print(\"\\nGenerating Experiment 1 plots...\")\n",
    "    plot_exp1_heatmap(df, 'success_rate', output_dir)\n",
    "    plot_exp1_heatmap(df, 'mean_return', output_dir)\n",
    "    plot_exp1_heatmap(df, 'crash_rate', output_dir)\n",
    "    plot_exp1_robustness_curves(df, output_dir)\n",
    "    print(\"✓ All Experiment 1 plots generated\")\n",
    "\n",
    "\n",
    "def create_all_exp2_plots(df, output_dir=Path(\"results/exp2/plots\")):\n",
    "    print(\"\\nGenerating Experiment 2 plots...\")\n",
    "    plot_exp2_performance(df, 'success_rate', output_dir)\n",
    "    plot_exp2_performance(df, 'crash_rate', output_dir)\n",
    "    plot_exp2_performance(df, 'mean_time_to_failure', output_dir)\n",
    "    plot_exp2_performance(df, 'mean_pre_fault_return', output_dir)\n",
    "    plot_exp2_performance(df, 'mean_post_fault_return', output_dir)\n",
    "    print(\"✓ All Experiment 2 plots generated\")\n",
    "\n",
    "print(\"✓ Visualization module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bc824",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e658331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Analysis module loaded\n"
     ]
    }
   ],
   "source": [
    "def analyze_exp1(df):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT 1: Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nBest training probability for each test level:\")\n",
    "    for test_p in FAILURE_PROBS:\n",
    "        best_train = df[df['test_p'] == test_p].groupby('train_p')['success_rate'].mean().idxmax()\n",
    "        best_rate = df[(df['test_p'] == test_p) & (df['train_p'] == best_train)]['success_rate'].mean()\n",
    "        print(f\"  Test p={test_p:.2f}: Best train p={best_train:.2f} (success={best_rate*100:.1f}%)\")\n",
    "    \n",
    "    on_dist = df[df['train_p'] == df['test_p']]['success_rate'].mean()\n",
    "    off_dist = df[df['train_p'] != df['test_p']]['success_rate'].mean()\n",
    "    print(f\"\\nGeneralization gap:\")\n",
    "    print(f\"  On-distribution:  {on_dist*100:.1f}%\")\n",
    "    print(f\"  Off-distribution: {off_dist*100:.1f}%\")\n",
    "    print(f\"  Gap: {(on_dist - off_dist)*100:.1f} pp\")\n",
    "\n",
    "\n",
    "def analyze_exp2(df):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT 2: Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    grouped = df.groupby('test_p')['success_rate'].mean()\n",
    "    critical_p = None\n",
    "    for p, rate in grouped.items():\n",
    "        if rate < 0.5:\n",
    "            critical_p = p\n",
    "            break\n",
    "    \n",
    "    if critical_p:\n",
    "        print(f\"\\nCritical failure threshold: p={critical_p*100:.0f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo critical failure threshold (robust up to 25%)\")\n",
    "    \n",
    "    p0 = df[df['test_p'] == 0.0]['success_rate'].mean()\n",
    "    p25 = df[df['test_p'] == 0.25]['success_rate'].mean()\n",
    "    print(f\"\\nPerformance degradation:\")\n",
    "    print(f\"  p=0%:  {p0*100:.1f}%\")\n",
    "    print(f\"  p=25%: {p25*100:.1f}%\")\n",
    "    print(f\"  Drop:  {(p0 - p25)*100:.1f} pp\")\n",
    "    \n",
    "    print(f\"\\nTime-to-failure analysis:\")\n",
    "    for p in FAILURE_PROBS:\n",
    "        ttf = df[df['test_p'] == p]['mean_time_to_failure'].mean()\n",
    "        print(f\"  p={p:.2f}: {ttf:.1f} steps\")\n",
    "\n",
    "\n",
    "def compare_experiments(df1, df2):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nZero-shot vs Matched training:\")\n",
    "    for test_p in FAILURE_PROBS:\n",
    "        exp2_rate = df2[df2['test_p'] == test_p]['success_rate'].mean()\n",
    "        exp1_rate = df1[(df1['train_p'] == test_p) & (df1['test_p'] == test_p)]['success_rate'].mean()\n",
    "        improvement = (exp1_rate - exp2_rate) * 100\n",
    "        print(f\"  p={test_p:.2f}: Zero-shot={exp2_rate*100:5.1f}%, \"\n",
    "              f\"Matched={exp1_rate*100:5.1f}%, Δ={improvement:+5.1f}pp\")\n",
    "\n",
    "print(\"✓ Analysis module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d7b50",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c90d2981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✓ ALL MODULES LOADED - CORRECTIONS APPLIED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def run_complete_pipeline():\n",
    "    \"\"\"Run complete research pipeline\"\"\"\n",
    "    \n",
    "    RUN_TRAINING_EXP1 = False\n",
    "    RUN_TRAINING_EXP2 = False\n",
    "    RUN_EVALUATION_EXP1 = True\n",
    "    RUN_EVALUATION_EXP2 = True\n",
    "    GENERATE_PLOTS = True\n",
    "    RUN_ANALYSIS = True\n",
    "    \n",
    "    if RUN_TRAINING_EXP1:\n",
    "        models_exp1 = train_experiment1(Path(\"models/exp1\"), verbose=1)\n",
    "    \n",
    "    if RUN_EVALUATION_EXP1:\n",
    "        models_exp1 = load_all_models_exp1(Path(\"models/exp1\"))\n",
    "        df_exp1 = evaluate_experiment1(models_exp1, N_EVAL_EPISODES, Path(\"results/exp1\"))\n",
    "        if GENERATE_PLOTS:\n",
    "            create_all_exp1_plots(df_exp1, Path(\"results/exp1/plots\"))\n",
    "        if RUN_ANALYSIS:\n",
    "            analyze_exp1(df_exp1)\n",
    "    \n",
    "    if RUN_TRAINING_EXP2:\n",
    "        models_exp2 = train_experiment2(Path(\"models/exp2\"), verbose=1)\n",
    "    \n",
    "    if RUN_EVALUATION_EXP2:\n",
    "        models_exp2 = load_all_models_exp2(Path(\"models/exp2\"))\n",
    "        df_exp2 = evaluate_experiment2(models_exp2, N_EVAL_EPISODES, \n",
    "                                       record_gifs=True, output_dir=Path(\"results/exp2\"))\n",
    "        if GENERATE_PLOTS:\n",
    "            create_all_exp2_plots(df_exp2, Path(\"results/exp2/plots\"))\n",
    "        if RUN_ANALYSIS:\n",
    "            analyze_exp2(df_exp2)\n",
    "    \n",
    "    if RUN_ANALYSIS and RUN_EVALUATION_EXP1 and RUN_EVALUATION_EXP2:\n",
    "        compare_experiments(df_exp1, df_exp2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"Quick test of corrected environment\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING CORRECTED ENVIRONMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for p in [0.0, 0.15, 0.25]:\n",
    "        print(f\"\\nTesting p={p:.2f}:\")\n",
    "        env = make_stochastic_lunar_lander(failure_prob=p, seed=42)\n",
    "        obs, info = env.reset()\n",
    "        print(f\"  Active family: {info.get('active_family')}\")\n",
    "        print(f\"  Failure occurred: {env.failure_occurred}\")\n",
    "        \n",
    "        for step in range(50):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            if env.failure_occurred and env.failure_step == step:\n",
    "                print(f\"  First failure at step {step}\")\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        env.close()\n",
    "        print(f\"  ✓ Episode complete (length={step+1})\")\n",
    "    \n",
    "    print(\"\\n✓ All tests passed!\")\n",
    "\n",
    "\n",
    "# Jupyter helpers\n",
    "def jupyter_train_exp1():\n",
    "    return train_experiment1(Path(\"models/exp1\"), verbose=1)\n",
    "\n",
    "def jupyter_train_exp2():\n",
    "    return train_experiment2(Path(\"models/exp2\"), verbose=1)\n",
    "\n",
    "def jupyter_evaluate_exp1():\n",
    "    models = load_all_models_exp1(Path(\"models/exp1\"))\n",
    "    df = evaluate_experiment1(models, N_EVAL_EPISODES, Path(\"results/exp1\"))\n",
    "    create_all_exp1_plots(df, Path(\"results/exp1/plots\"))\n",
    "    analyze_exp1(df)\n",
    "    return df\n",
    "\n",
    "def jupyter_evaluate_exp2(record_gifs=True):\n",
    "    models = load_all_models_exp2(Path(\"models/exp2\"))\n",
    "    df = evaluate_experiment2(models, N_EVAL_EPISODES, record_gifs, Path(\"results/exp2\"))\n",
    "    create_all_exp2_plots(df, Path(\"results/exp2/plots\"))\n",
    "    analyze_exp2(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL MODULES LOADED - CORRECTIONS APPLIED\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbca3e4",
   "metadata": {},
   "source": [
    "The cell below has already been executed and the results were saved!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING AUTOMATED EXECUTION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nWhat would you like to run?\")\n",
    "    print(\"1. Quick test (verify corrections work)\")\n",
    "    print(\"2. Train Experiment 1 (6-10 hours, 30 agents)\")\n",
    "    print(\"3. Train Experiment 2 (2-3 hours, 5 agents)\")\n",
    "    print(\"4. Evaluate Experiment 1 (requires trained models)\")\n",
    "    print(\"5. Evaluate Experiment 2 (requires trained models)\")\n",
    "    print(\"6. Full analysis (requires results CSVs)\")\n",
    "    print(\"7. Complete pipeline (train + evaluate everything)\")\n",
    "    \n",
    "    # AUTO-RUN: Quick test by default to show it works\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AUTO-RUNNING: QUICK TEST\")\n",
    "    print(\"=\"*70)\n",
    "    quick_test()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QUICK TEST COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nTo run full experiments, uncomment the section below:\")\n",
    "    print(\"Or use these commands in a new cell:\")\n",
    "    print(\"  jupyter_train_exp1()      # Train experiment 1\")\n",
    "    print(\"  jupyter_train_exp2()      # Train experiment 2\")\n",
    "    print(\"  jupyter_evaluate_exp1()   # Evaluate experiment 1\")\n",
    "    print(\"  jupyter_evaluate_exp2()   # Evaluate experiment 2\")\n",
    "    print(\"  run_complete_pipeline()   # Run everything\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RUNNING COMPLETE PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Experiment 1: Train 30 agents\n",
    "    print(\"\\n### EXPERIMENT 1: TRAINING ###\")\n",
    "    models_exp1 = train_experiment1(Path(\"models/exp1\"), verbose=1)\n",
    "    \n",
    "    # Experiment 1: Evaluate\n",
    "    print(\"\\n### EXPERIMENT 1: EVALUATION ###\")\n",
    "    df_exp1 = evaluate_experiment1(models_exp1, N_EVAL_EPISODES, Path(\"results/exp1\"))\n",
    "    create_all_exp1_plots(df_exp1, Path(\"results/exp1/plots\"))\n",
    "    analyze_exp1(df_exp1)\n",
    "    \n",
    "    # Experiment 2: Train 5 agents\n",
    "    print(\"\\n### EXPERIMENT 2: TRAINING ###\")\n",
    "    models_exp2 = train_experiment2(Path(\"models/exp2\"), verbose=1)\n",
    "    \n",
    "    # Experiment 2: Evaluate\n",
    "    print(\"\\n### EXPERIMENT 2: EVALUATION ###\")\n",
    "    df_exp2 = evaluate_experiment2(models_exp2, N_EVAL_EPISODES, \n",
    "                                   record_gifs=True, output_dir=Path(\"results/exp2\"))\n",
    "    create_all_exp2_plots(df_exp2, Path(\"results/exp2/plots\"))\n",
    "    analyze_exp2(df_exp2)\n",
    "    \n",
    "    # Comparative analysis\n",
    "    print(\"\\n### COMPARATIVE ANALYSIS ###\")\n",
    "    compare_experiments(df_exp1, df_exp2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓✓✓ COMPLETE PIPELINE FINISHED ✓✓✓\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nResults saved to:\")\n",
    "    print(\"  models/exp1/          - 30 trained agents\")\n",
    "    print(\"  models/exp2/          - 5 trained agents\")\n",
    "    print(\"  results/exp1/         - Cross-evaluation results\")\n",
    "    print(\"  results/exp1/plots/   - Heatmaps and curves\")\n",
    "    print(\"  results/exp2/         - Zero-shot results\")\n",
    "    print(\"  results/exp2/plots/   - Performance plots\")\n",
    "    print(\"  results/exp2/gifs/    - Evaluation videos\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c705776",
   "metadata": {},
   "source": [
    "Fixing the error above using the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cd5a8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESUMING FROM PLOT GENERATION (11 hours of work preserved)\n",
      "======================================================================\n",
      "\n",
      "### REGENERATING PLOTS (Exp2) ###\n",
      "\n",
      "Generating Experiment 2 plots...\n",
      "✓ All Experiment 2 plots generated\n",
      "\n",
      "### RUNNING ANALYSIS ###\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: Analysis\n",
      "======================================================================\n",
      "\n",
      "Critical failure threshold: p=10%\n",
      "\n",
      "Performance degradation:\n",
      "  p=0%:  51.0%\n",
      "  p=25%: 44.0%\n",
      "  Drop:  7.0 pp\n",
      "\n",
      "Time-to-failure analysis:\n",
      "  p=0.00: 588.5 steps\n",
      "  p=0.05: 587.7 steps\n",
      "  p=0.10: 568.7 steps\n",
      "  p=0.15: 525.1 steps\n",
      "  p=0.20: 487.3 steps\n",
      "  p=0.25: 486.7 steps\n",
      "\n",
      "### COMPARATIVE ANALYSIS ###\n",
      "\n",
      "======================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Zero-shot vs Matched training:\n",
      "  p=0.00: Zero-shot= 51.0%, Matched= 51.0%, Δ= +0.0pp\n",
      "  p=0.05: Zero-shot= 50.0%, Matched= 33.6%, Δ=-16.4pp\n",
      "  p=0.10: Zero-shot= 49.0%, Matched= 32.6%, Δ=-16.4pp\n",
      "  p=0.15: Zero-shot= 47.2%, Matched= 39.2%, Δ= -8.0pp\n",
      "  p=0.20: Zero-shot= 47.0%, Matched= 33.4%, Δ=-13.6pp\n",
      "  p=0.25: Zero-shot= 44.0%, Matched= 31.8%, Δ=-12.2pp\n",
      "\n",
      "======================================================================\n",
      "✓✓✓ COMPLETE PIPELINE FINISHED ✓✓✓\n",
      "======================================================================\n",
      "\n",
      "Results saved to:\n",
      "  models/exp1/          - 30 trained agents\n",
      "  models/exp2/          - 5 trained agents\n",
      "  results/exp1/         - Cross-evaluation results\n",
      "  results/exp1/plots/   - Heatmaps and curves\n",
      "  results/exp2/         - Zero-shot results\n",
      "  results/exp2/plots/   - Performance plots ✓ FIXED\n",
      "  results/exp2/gifs/    - Evaluation videos\n"
     ]
    }
   ],
   "source": [
    "# FIX: Create directories BEFORE saving plots, then continue analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMING FROM PLOT GENERATION (11 hours of work preserved)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create missing directories\n",
    "Path(\"results/exp2/plots\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"results/exp1/plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Regenerate the plots that failed\n",
    "print(\"\\n### REGENERATING PLOTS (Exp2) ###\")\n",
    "create_all_exp2_plots(df_exp2, Path(\"results/exp2/plots\"))\n",
    "\n",
    "print(\"\\n### RUNNING ANALYSIS ###\")\n",
    "analyze_exp2(df_exp2)\n",
    "\n",
    "# Comparative analysis\n",
    "print(\"\\n### COMPARATIVE ANALYSIS ###\")\n",
    "compare_experiments(df_exp1, df_exp2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓✓✓ COMPLETE PIPELINE FINISHED ✓✓✓\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nResults saved to:\")\n",
    "print(\"  models/exp1/          - 30 trained agents\")\n",
    "print(\"  models/exp2/          - 5 trained agents\")\n",
    "print(\"  results/exp1/         - Cross-evaluation results\")\n",
    "print(\"  results/exp1/plots/   - Heatmaps and curves\")\n",
    "print(\"  results/exp2/         - Zero-shot results\")\n",
    "print(\"  results/exp2/plots/   - Performance plots ✓ FIXED\")\n",
    "print(\"  results/exp2/gifs/    - Evaluation videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74886b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
